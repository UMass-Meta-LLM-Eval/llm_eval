/home/amansinghtha_umass_edu/.conda/envs/llm-parsing/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/amansinghtha_umass_edu/.conda/envs/llm-parsing/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(

NVIDIA A100-PCIE-40GB
Pipeline Generated!
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/99
1/10
1/10
1/10
1/10
1/10
1/10
1/10
1/10
1/10
1/10
1/4
1/4
1/4
1/4

